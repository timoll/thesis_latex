\chapter{Introduction}

\label{chap:introduction}
%\section{Eye Tracking}
Eye-tracking is utilized to monitor eye movement. It is employed in a wide variety of fields such as marketing research, psychology, virtual reality and sports. In sport research eye-tracking offers important insights about the hand-eye data or visual search techniques. These differ greatly between professional athletes and beginners so the analysis can improve the efficiency of training. 

An eye-tracking device for sports needs a high accuracy and temporal resolution to draw accurate conclusions. To interfere as little as possible with the activity of an athlete, it should also be portable. 

\section{Current Development}
In collaboration with the sport research institute at the University of Bern, the microLab research group has developed an eye-tracking system for sports. This system called Gazelle uses two cameras per eye to capture infrared images at a high-speed of 120 frames per second. The information from the cameras should be used to determine the gaze direction of the athlete. An important part of that is the detection of the pupil, which is not implemented satisfactory yet.

\section{Overview of the Eye-tracking System}
The Gazelle eye-tracking system consists of two main parts. The first is the GazelleGlasses where the cameras are mounted and the data are sent down to the second main part that is GazelleCompute. As the name suggests this component is responsible for the processing of the data.

\subsubsection{GazelleGlasses}
GazelleGlasses are glasses that an athlete can wear without obstructing the activity.
For the capture of an eye two cameras are placed below the eye and besides the nose. This positioning was chosen to not obstruct the view of the athlete and offer a good perspective on the eye. A front facing camera is placed between the eyes and provides a view that is close to the view of the athlete. The cameras are equipped with a filter to let infrared light through. This produces a better picture of the pupil. Infrared LEDs illuminate the eye for a brighter picture.

Data that are generated by the cameras need to be transferred to GazelleCompute. This is done by serializing the data and sending it over an Ethernet cable to the main compute unit.
\subsubsection{GazelleCompute}
The main processing is done on a small portable unit. The units main components are a Zynq FPGA and a Tegra 3. Data from the glasses is deserialised and read by an FPGA. Initially the idea was to do the image processing on the Tegra 3 as it is a powerful mobile System on Chip.

Problems with the data transfer from the FPGA to the Tegra 3 scrambled that idea. The image processing is now required to run on the dual core ARM CPU that is on the Zynq FPGA.

\section{Find the Gaze Direction}
The gaze direction should be determined With the image data from GazelleGlasses. This consists of the digital detection of the pupil as an ellipse for each camera. It needs to be very fast because it has to run on a dual core arm CPU with limited processing power. The algorithm for a stable detection of the pupil is described in chapter \ref{chap:pupildetection}. A second algorithm is applied in a partner work that calculates the gaze direction with the two ellipses from one eye. 

Reproducible test data is needed to verify the functionality of the whole process. Generating such data from real captures of the cameras where the gaze point is known would be ideal. This is not possible as the images can't be transferred to the Tegra 3 where the processing power to encode the video stream would be available. An alternative is an accurate model of the eye and the glasses. This model is presented in chapter \ref{chap:Model}. It allows to simply generate animations where the rotation of the eyes and many additional properties can be controlled. The output is pictures that reassemble captures of the actual cameras. Additionally the parameters of every object is logged for every frame that is rendered.

The methods to calculate the gaze direction where tested with the pictures as input and the parameters as reference. The influence of different resolutions and algorithms is investigated in chapter \ref{chap:results}


